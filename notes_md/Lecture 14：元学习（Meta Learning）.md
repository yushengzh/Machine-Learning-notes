[TOC]

## Lecture 14ï¼šå…ƒå­¦ä¹ ï¼ˆMeta Learningï¼‰

> Lectured by HUNG-YI LEE (æå®æ¯…)
> Recorded by Yusheng zhaoï¼ˆyszhao0717@gmail.comï¼‰

----------

> metaçš„æ„å‘³ï¼šmeta-X = X about X
> meta learningï¼š**å­¦ä¹ **å¦‚ä½•å­¦ä¹ 

DeepLearningå¤§éƒ¨åˆ†æ—¶é—´éƒ½æ˜¯åœ¨çˆ†è°ƒè¶…å‚æ•°ï¼Œå·¥ä¸šç•Œçš„æ–¹æ³•ï¼šå¤§åŠ›å‡ºå¥‡è¿¹ï¼Œæ‹¿å¥½å¤šå¼ GPUåŒæ—¶è·‘å‡ ç»„ä¸åŒçš„è¶…å‚æ•°ç»„ï¼Œçœ‹çœ‹å“ªä¸ªæœ€å¥½ç”¨ã€‚å­¦æœ¯ç•Œï¼ˆå­¦æ ¡çš„å®éªŒå®¤é‡Œè¾¹ï¼‰å¾€å¾€èµ„æºè´«ç©·/(ã„’oã„’)/~~

<img src="https://s3.bmp.ovh/imgs/2022/05/03/49a99dccc73e088f.png" style="zoom:67%;" />

æ‰€ä»¥æˆ‘ä»¬æƒ³ï¼Œæ—¢ç„¶æœºå™¨å­¦ä¹ å¯ä»¥è‡ªåŠ¨å­¦ä¸€ä¸ªæ¨¡å‹ï¼Œé‚£ä¹ˆhyper parameterå¯ä¸å¯ä»¥ä¹Ÿç”¨å­¦çš„å‘¢ï¼Ÿâ€”â€”è¿™å°±æ˜¯meta learningæ‰€åšçš„äº‹æƒ…ã€‚

> Machine Learningçš„çŸ¥è¯†å›é¡¾ï¼ˆäº‹å®ä¸Šï¼Œmeta learningå’Œmachine learningæ²¡æœ‰å¤ªå¤§åŒºåˆ«ï¼‰
> **ä¸‰ä¸ªæ­¥éª¤ï¼š**ï¼ˆç›®çš„ï¼šlooking for a functionï¼‰
>
> - **step 1ï¼šFunction with unknown**
> å…¶ä¸­ç¥ç»å…ƒçš„æƒé‡ï¼ˆweightsï¼‰å’Œåç½®ï¼ˆbiasesï¼‰å°±æ˜¯éœ€è¦å­¦ä¹ å¾—åˆ°çš„unknownçš„å‚æ•°ï¼Œç”¨$\theta$æ¥æ ‡è¯†
> - **step 2ï¼šDefine loss function**
> $L(\theta) = \sum^n_{i=1}e_i$ï¼Œå…¶ä¸­æ¯ä¸€ä¸ª$e_i$éƒ½æ˜¯trainç»“æœå’Œground truthä¹‹é—´çš„è·ç¦»ï¼ˆå¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡å°±æ˜¯äº¤å‰ç†µï¼‰
> - **step 3ï¼šOptimization**
> æ‰¾ä¸€ä¸ª$\theta ^*$ä½¿å¾—lossè¶Šå°è¶Šå¥½ï¼Œå³ä¼˜åŒ–ä»»åŠ¡ï¼š$\theta^* = arg \ \underset{\theta}{min}L(\theta)$.
> æœ¬è¯¾ç¨‹ä¸­éƒ½æ˜¯ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥è§£å†³è¿™ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ç»„lossè¶³å¤Ÿä¸‹çš„å‚æ•°ç»„$\theta^*$ï¼Œé‚£ä¹ˆå‚æ•°å¸¦å…¥é»‘ç®±å‡½æ•°$f_{\theta^*}$ä¸­ï¼Œå®ç°æˆ‘ä»¬éœ€è¦çš„ç«¯åˆ°ç«¯çš„ä»»åŠ¡ï¼ˆè¾“å…¥-è¾“å‡ºï¼‰

### Introduction of Meta Learning

ä¸€ä¸ªMLç®—æ³•â€œç®€åŒ–â€æ¥çœ‹ä¹Ÿæ˜¯ä¸€ä¸ªfunctionï¼Œè¿™ä¸ªfunctionçš„è¾“å…¥æ˜¯ä¸€ä¸ªæ•°æ®é›†ï¼ˆtraining exampleï¼‰ï¼Œè¾“å‡ºè®­ç»ƒçš„ç»“æœï¼ˆå¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡ï¼‰é‚£å°±æ˜¯ä¸€ä¸ªclassifierï¼›æŠŠtest setæµ‹è¯•é›†ä¸¢è¿›è¿™ä¸ªclassifierä¸­ï¼Œè¿™ä¸ªç®—æ³•çš„æœŸæœ›å½“ç„¶å°±æ˜¯åˆ†ç±»æ­£ç¡®ç‡è¶Šé«˜è¶Šå¥½ã€‚

<img src="https://s3.bmp.ovh/imgs/2022/05/03/03d2380c39bed1df.png" style="zoom:67%;" />

è¿™ä¸ªç®—æ³•$F$é€šå¸¸æ˜¯Hand-craftedï¼ˆäººæƒ³å‡ºæ¥çš„ï¼‰ï¼Œæˆ‘ä»¬ä»¥ä¸‹å€Ÿé‰´MLçš„ä¸‰ä¸ªæ­¥éª¤æ¥å­¦ä¹ è¿™ä¸ª$F$ã€‚

#### Step 1

> MLé‡Œè¾¹çš„step 1ï¼Œå…¶ä¸­learnableçš„æ˜¯neuronç¥ç»å…ƒçš„weightå’Œbiases

åœ¨meta learningé‡Œè¾¹å¯ä»¥å­¦å‡ºæ¥çš„ä¸œè¥¿â€”â€”*ç½‘ç»œæ¶æ„ï¼ˆNet Architectureï¼‰*ã€*åˆå§‹åŒ–å‚æ•°ï¼ˆInitial patametersï¼‰*ã€*å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰*ç­‰ã€‚ä»¥ä¸Šä¹‹å‰è¯¾ç¨‹ä¸­æˆ‘ä»¬éƒ½æ˜¯äººä¸ºè®¾ç½®ï¼Œç°åœ¨å¸Œæœ›ä½¿ç”¨meta learningæ¥è¿›è¡Œå­¦ä¹ ã€‚

- ç”¨ $\phi$ æ¥ç»Ÿç§°éœ€è¦å…ƒå­¦ä¹ çš„æˆåˆ†ï¼ˆlearnable componentsğŸ‘†ï¼‰ï¼š*ç½‘ç»œæ¶æ„ï¼ˆNet Architectureï¼‰*ã€*åˆå§‹åŒ–å‚æ•°ï¼ˆInitial patametersï¼‰*ã€*å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰*ç­‰

- ä»¥ä¸‹éƒ½æŠŠlearning algorithmè®°ä½œ$F_\phi$ï¼Œ$\phi$ä»£è¡¨äº†æœªçŸ¥çš„å‚æ•°æ˜¯

- ä¸åŒçš„meta learningçš„æ–¹æ³•å…¶å®å°±æ˜¯æƒ³åŠæ³•å»å­¦ä¸åŒçš„component

  Categorize meta learning based on what is learnable	$\Rightarrow \ \phi$

#### Step 2

- é’ˆå¯¹<u>***learning algorithm***</u> $F_\phi$å®šä¹‰<u>***loss function***</u>
  è¿™ä¸ªloss functionè®°ä½œ$L(\phi)$ï¼Œå¦‚æœ$L(\phi)$æ¯”è¾ƒå°ï¼Œè¯´æ˜è¿™ä¸ª$F(\phi)$æ¯”è¾ƒå¥½ã€‚<img src="https://s1.328888.xyz/2022/05/21/dqtCS.png" alt="image-20220516211820383" style="zoom:25%;" />
- æˆ‘ä»¬ç”¨è®­ç»ƒä»»åŠ¡ï¼ˆtraining tasksï¼‰æ¥ä½œä¸ºè®­ç»ƒèµ„æ–™å–‚ç»™meta learningçš„æ¨¡å‹$F_\phi$ã€‚å¦‚ä¸‹å›¾

<img src="https://s1.328888.xyz/2022/05/21/dq5bR.png" style="zoom:67%;" />

å¦‚ä¸Šï¼Œä»¥è®­ç»ƒäºŒå…ƒåˆ†ç±»å™¨ä¸ºä¾‹ï¼Œæ¯ä¸ªä»»åŠ¡é‡Œé¢éƒ½æœ‰è®­ç»ƒèµ„æ–™å’Œæµ‹è¯•èµ„æ–™ã€‚

- å®šä¹‰$L(\phi)$ï¼šæŠŠæŸä¸€ä¸ªä»»åŠ¡çš„èµ„æ–™æ‹¿å‡ºæ¥ä¸¢ç»™learning algorithm $F_\phi$ï¼Œè¾“å‡ºä¸€ä¸ªå…·ä½“çš„åˆ†ç±»å™¨ï¼ˆoutputï¼‰ï¼Œä»»åŠ¡ä¸€çš„classifierè®°ä½œ$\Large f_{\theta^{1*}}$

  <img src="https://s1.328888.xyz/2022/05/21/dqlXA.png" style="zoom:67%;" />

- ç¡®å®šclassifer$\Large f_{\theta^{1*}}$çš„æ€§èƒ½å¥½åï¼šç”¨ä»»åŠ¡çš„æµ‹è¯•èµ„æ–™å¯¹è¯¥åˆ†ç±»å™¨è¿›è¡Œè¯„ä¼°

<img src="https://s1.328888.xyz/2022/05/21/dqzyi.png" style="zoom:67%;" />

â€‹		å¦‚ä¸Šå›¾ï¼Œæµ‹è¯•èµ„æ–™ä¸¢è¿›è¿™ä¸ªclassifieråšä¸€ä¸ªpredictionï¼Œè®¡ç®—ï¼ˆé¢„æµ‹å’Œground truthï¼‰äº¤å‰ç†µç»Ÿç»ŸåŠ èµ·æ¥å¾—åˆ°$l^1$

- ç±»ä¼¼å¦‚ä¸Šè¿‡ç¨‹ï¼Œç”¨å…¶ä»–ä»»åŠ¡æ¥ç¡®å®šå„è‡ªçš„classifierï¼ˆè¿™ä¸ªä¾‹å­ä¸­meta learningåªæœ‰ä¸¤ä¸ªä»»åŠ¡ï¼‰

  <img src="https://s1.328888.xyz/2022/05/21/dqCdv.png" style="zoom:67%;" />

  åœ¨ä»»åŠ¡ä¸€å’Œä»»åŠ¡äºŒçš„è¡¨ç°åˆ†åˆ«ä¸º $l^1$ å’Œ $l^2$ï¼Œå°†ä¸¤è€…åŠ èµ·æ¥ï¼Œå¾—åˆ°æ€»lossä¸º$l^1 + l^2$ï¼Œå¯¹äºnä¸ªä»»åŠ¡çš„meta learningæ¥è¯´
  $$
  L(\phi) = \sum_{i=1}^nl^i
  $$

  > åœ¨ä¸€èˆ¬çš„MLä¸­ï¼Œæˆ‘ä»¬ç”¨è®­ç»ƒèµ„æ–™è®¡ç®—lossï¼Œè€Œåœ¨meta Learningä¸­æˆ‘ä»¬ç”¨æµ‹è¯•èµ„æ–™æ¥è®¡ç®—lossã€‚è¿™æ˜¯å› ä¸ºmeta learningçš„è®­ç»ƒå•ä½æ˜¯â€œtraining taskâ€ï¼Œæ¢è¨€ä¹‹ï¼Œè¯„ä¼°meta learningçš„æ€§èƒ½æ˜¯åŸºäºâ€œtesting taskâ€ä¸Šè¡¨ç°å¦‚ä½•ï¼Œåœ¨å•ä¸€è®­ç»ƒå•å…ƒï¼ˆtasksï¼‰ä¸Šï¼Œè®¡ç®—losså¯ä»¥é‡‡ç”¨æµ‹è¯•èµ„æ–™ã€‚è€Œtypical MLçš„è¯„ä¼°åˆ™æ˜¯æ ¹æ®æµ‹è¯•èµ„æ–™ä¸Šçš„ç»“æœï¼Œå› è€Œä¸èƒ½ç”¨æµ‹è¯•èµ„æ–™æ¥è®¡ç®—lossã€‚
  >
  > <img src="https://s1.328888.xyz/2022/05/21/dqpK0.png" style="zoom:70%;" />

#### Step 3

- å¯¹äºlearning algorithm$F(\phi)$å·²çŸ¥loss function$L(\phi) = \sum_{i=1}^nl^i$
- æœ¬æ­¥éª¤ç›®çš„ï¼šæ‰¾åˆ°ä¸€ä¸ª$\phi$å»minimize$L(\phi)$ï¼Œå³ä¼˜åŒ–é—®é¢˜ï¼š$\phi^* = arg \ \underset{\phi}{min} \ L(\phi)$

- è§£è¿™ä¸ªä¼˜åŒ–é—®é¢˜
  - å¦‚æœçŸ¥é“losså¯¹$\phi$åå¯¼ï¼Œå³$\large \frac{\part L}{\part \phi}$æ˜“äºè®¡ç®—ï¼Œé‚£å°±ç”¨æ¢¯åº¦ä¸‹é™
  - ç»å¸¸çš„æƒ…å†µï¼Œåœ¨metaé‡Œè¾¹ï¼Œlossçš„åå¯¼ä¸æ˜“äºè®¡ç®—ã€‚è¿™æ—¶å€™éœ€è¦**Reinforcement Learning**ç¡¬trainä¸€å‘ï¼Œæˆ–è€…ç”¨è¿›åŒ–ç®—æ³•ï¼ˆEvolutionary Algorithmï¼‰

ç»è¿‡ä»¥ä¸Šä¸‰æ­¥ï¼Œæœ€ç»ˆæˆ‘ä»¬learnedå‡ºæ¥ä¸€ä¸ªlearning algorithm$F_\phi$ã€‚

#### æ€»ç»“ï¼šmeta learningçš„æ¡†æ¶

<img src="https://s1.328888.xyz/2022/05/21/dq0PJ.png" style="zoom:67%;" />

ç®€è€Œè¨€ä¹‹ï¼Œmeta learningå°±æ˜¯åœ¨*è®­ç»ƒä»»åŠ¡*ä¸Šæ‰¾å‡ºç±»æ¯”äººç±»æƒ³å‡ºæ¥ï¼ˆè­¬å¦‚SGDç­‰ç­‰ï¼‰çš„**learning algorithm**ï¼Œå†å°†è¯¥**ä¼˜åŒ–æ–¹æ³•**ç”¨åœ¨*æµ‹è¯•ä»»åŠ¡*ä¸Šï¼Œä»è€Œå¾—åˆ°è¾ƒå¥½è¡¨ç°çš„ç›®æ ‡å‡½æ•°ã€‚

> few shot learningï¼Œæœ‰äººè§‰å¾—å’Œmeta learningå¾ˆåƒã€‚å› ä¸ºå°æ ·æœ¬å­¦ä¹ ï¼ˆfew shot learningï¼‰çš„learning algorithmï¼ˆé€šå¸¸æ˜¯ä¸æ˜“äººç±»æƒ³å‡ºæ¥çš„ï¼‰åŸºæœ¬éƒ½æ˜¯é€šè¿‡meta learningå¾—åˆ°çš„ã€‚æ¢è¨€ä¹‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡meta learningå®ç°few shot learningã€‚
>
> å¯¹äºæ•´ä¸ªmeta learningçš„æ¡†æ¶è€Œè¨€â€œtraining dataâ€æŒ‡çš„å°±æ˜¯training taskï¼Œå…·ä½“åˆ°testing taskä¸­ï¼Œâ€œtraining dataâ€æŒ‡çš„å°±æ˜¯learning algorithmç”¨åœ¨testing taskæ‰€åšMLä»»åŠ¡çš„å¸¸è§„æ„ä¹‰çš„è®­ç»ƒæ•°æ®ã€‚

### æ¯”è¾ƒï¼šMachine Learning v.s. Meta Learning

> ä»¥ä¸‹è‹¥å¹²æ–¹é¢çš„æ¯”è¾ƒ

#### Goal

<img src="https://s1.328888.xyz/2022/05/21/dq7AF.png" style="zoom: 80%;" />

å…¸å‹çš„MLä»»åŠ¡æ˜¯ä¸ºäº†æ‰¾åˆ°ä¸€ä¸ªblack-boxçš„ç›®æ ‡å‡½æ•°ï¼Œè€ŒMetaåˆ™æ˜¯ä¸ºäº†æ‰¾åˆ°èƒ½æ‰¾åˆ°è¿™ä¸ªç›®æ ‡å‡½æ•°$f$çš„ä¼˜åŒ–æ–¹æ³•$F$ã€‚

#### Training Data

<img src="https://s1.328888.xyz/2022/05/21/dqIYW.png" style="zoom:67%;" />

MLï¼šè®­ç»ƒèµ„æ–™ã€æµ‹è¯•èµ„æ–™
Metaï¼šè®­ç»ƒä»»åŠ¡ã€æµ‹è¯•ä»»åŠ¡ï¼ˆå…¶ä¸­çš„è®­ç»ƒèµ„æ–™ç§°ä¹‹ä¸ºSupport Setã€æµ‹è¯•èµ„æ–™ç§°ä¹‹ä¸ºQuery Setï¼‰

#### within-task training/testing v.s. across-task training/testing

<img src="https://s1.328888.xyz/2022/05/21/dqTXk.png" style="zoom: 67%;" />

ä¸€èˆ¬çš„MLæ˜¯**Within-task Training**ï¼Œè€Œmetaé‡Œè¾¹æ ¹æ®ä¸€å †ä»»åŠ¡å­¦å‡ºä¸€ä¸ªlearning algorithmå«åš**Across-task Training**ã€‚

<img src="https://s1.328888.xyz/2022/05/21/dqcpd.png" style="zoom:67%;" />

å­¦ä¹ å‡ºæ¥çš„ä¸œè¥¿ï¼ˆåˆ†ç±»å™¨/learning algorithmï¼‰å¦‚ä½•å¤„ç†ï¼Ÿåœ¨MLä¸­æ˜¯test dataä¸¢è¿›å»ï¼Œè·‘å‡ºæ¥ç»“æœï¼Œç»“æŸã€‚è¿™ä¸ªæµç¨‹å°±æ˜¯**within-task testing**ã€‚è€Œåœ¨metaä¸­ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå­¦ä¹ å‡ºæ¥çš„ä¼˜åŒ–æ–¹æ³•æ”¾è¿›test taskåšä¸€æ¬¡å¸¸è§„çš„MLï¼ŒåŒ…æ‹¬within-task trainingå’Œtestingï¼ˆå¦‚ä¸Šå›¾å„ä¸€æ¬¡ï¼‰ï¼Œè¿™ä¸¤ä¸ªæµç¨‹æœ‰æ—¶ä¹Ÿç§°ä¹‹ä¸º***Episode***ã€‚æ•´ä¸ªmetaè¿™éƒ¨åˆ†æµç¨‹ç§°ä¹‹ä¸º**Across-task Testing**

#### Loss

<img src="https://s1.328888.xyz/2022/05/21/dqmFQ.png" style="zoom:67%;" />

MLï¼šä¸€ä¸ªä»»åŠ¡ä¸­ï¼Œä¸åŒæ•°æ®å•å…ƒçš„lossä¹‹å’Œï¼›Metaï¼šä¸€æŠŠä»»åŠ¡ï¼Œæ¯ä¸ªè®­ç»ƒä»»åŠ¡ä¸­çš„æµ‹è¯•æ•°æ®ç»¼åˆlossä¹‹å’Œ

#### trainingè¿‡ç¨‹

è¦ç®—æ¯ä¸ªä»»åŠ¡çš„$l$ï¼Œéœ€è¦ç»è¿‡ä¸€æ¬¡**Within-task Training**ã€ä¸€æ¬¡**within-task testing**

<img src="https://s1.328888.xyz/2022/05/21/dq3y3.png" style="zoom:67%;" />

åœ¨metaä¸€ä¸ªæµç¨‹ï¼ˆå³Across-task trainingï¼‰ä¸­åŒ…å«è‹¥å¹²ä¸ªwithin-task training/testingã€‚åœ¨ä¸€äº›æ–‡çŒ®ä¸­ï¼Œä¸¤è€…åˆ†åˆ«ç§°ä¹‹ä¸º***Outer Loop***ï¼ˆAcross-task trainingï¼‰å’Œ***Inner Loop***ï¼ˆwithin-task training/testingï¼‰

#### Metaå’ŒMLçš„ç›¸ä¼¼å¤„

- training tasksä¸Šçš„è¿‡æ‹Ÿåˆ
- æ‹¿æ›´å¤šçš„è®­ç»ƒä»»åŠ¡æ¥è®­ç»ƒä½ çš„æ¨¡å‹ï¼Œæé«˜meta learningæœ€ååœ¨test taskçš„æ€§èƒ½ï¼ˆperformanceï¼‰
- â€œä»»åŠ¡å¢å¼ºâ€ï¼ˆTask Augmentationï¼‰
- learning algorithmä¸­çš„ä¹Ÿæœ‰è®¸å¤šè¶…å‚æ•°ï¼Œè¯¥æ­»å±…ç„¶è¿˜æœ‰è°ƒå‚â€¦â€¦å¥—å¨ƒçˆ†è°ƒå‚ï¼ˆğŸ˜Ÿéš¾é“metaä¸å°±æ˜¯ä¸ºäº†æ–°ä»»åŠ¡æ–°æ¨¡å‹ä¸æµªè´¹æ—¶é—´è°ƒå‚éº½ï¼‰

- Development task ğŸ˜Šï¼ˆ~~ç±»æ¯”development setï¼‰

  > **å¼€å‘é›†ï¼ˆdevelopment setï¼‰**ç”¨äºè°ƒæ•´å‚æ•°ï¼Œé€‰æ‹©ç‰¹å¾ï¼Œä»¥åŠå¯¹å­¦ä¹ ç®—æ³•ä½œå‡ºå…¶å®ƒå†³å®šã€‚æœ‰æ—¶ä¹Ÿç§°ä¸º**ç•™å‡ºäº¤å‰éªŒè¯é›†ï¼ˆhold-out cross validation setï¼‰**ã€‚åœ¨supervised MLä¸­ç»å¸¸ç”¨äºç¡®å®šç½‘ç»œç»“æ„æˆ–è€…æ§åˆ¶æ¨¡å‹å¤æ‚ç¨‹åº¦çš„å‚æ•°ã€‚

  å¾ˆå¤šmetaçš„è®ºæ–‡å®é™…ä¸Šéƒ½æ²¡æœ‰ä½¿ç”¨development taskï¼ˆç”¨æ¥è°ƒæ•´learning algorithmçš„å¤æ‚ç¨‹åº¦ï¼‰ï¼Œæˆ–è®¸è¿™ä¹Ÿæ˜¯å¯ä»¥åšä¸€åšçš„ç‚¹ã€‚

### å®ä¾‹è¯´æ˜ï¼šlearning algorithmä¸­å“ªäº›æ˜¯å¯ä»¥è¢«â€œå­¦ä¹ â€çš„ï¼Ÿ

> åœ¨æ¢¯åº¦ä¸‹é™ä¸­ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªNetwork Structureï¼›æ ¹æ®training dataä¸æ–­æ›´æ–°æ¢¯åº¦ï¼Œç›´åˆ°å¾—åˆ°æ»¡æ„çš„ç»“æœã€‚
>
> <img src="https://s1.328888.xyz/2022/05/21/dqKl4.png" style="zoom:67%;" />
>
> ä»¥ä¸Šï¼Œåˆå§‹åŒ–å‚æ•°æ˜¯å¯ä»¥è¢«å­¦ä¹ çš„ã€‚

#### Learning to Initialize

- Model-Agnostic Meta-Learning (MAML)

  > Chelsea Finn, Pieter Abbeel, and Sergey Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networksâ€, ICML, 2017

  - How to train MAML
    > Antreas Antoniou, Harrison Edwards, Amos Storkey, How to train your MAML, ICLR, 2019
    
    éœ€è¦è°ƒå‚ï¼Œrandom seed

    <img src="https://s1.328888.xyz/2022/05/21/dqOKB.png" style="zoom:50%;" />

  - è”æƒ³åˆ°Pre-trainingã€lifelong learnigå’Œtransfer learning

    åœ¨MAMLé‡Œé¢ï¼Œé€šè¿‡ä¸€æŠŠå­è®­ç»ƒä»»åŠ¡æ‰¾åˆ°å¥½çš„initï¼Œæœ€åç”¨åœ¨æµ‹è¯•ä»»åŠ¡ä¸Šã€‚åœ¨self-supervised learningé‡Œé¢ï¼ˆä¹Ÿæœ‰ç±»ä¼¼çš„åšæ³•ï¼‰ï¼Œåœ¨proxy tasksä¸Šè®­ç»ƒï¼Œæœ€åç”¨åœ¨æµ‹è¯•æ•°æ®ï¼Œè­¬å¦‚BERTå°±æ˜¯åšå¥å­çš„å¡«ç©ºã€ä¹Ÿæœ‰ä¸€äº›å·¥ä½œå¯ä»¥åšå›¾åƒåƒç´ çš„å¡«ç©ºï¼ˆmaskingï¼‰ã€‚~~kaimingçš„MAEä¸€æ ·çš„æ€è·¯~~

    <img src="https://s1.328888.xyz/2022/05/21/dqaPT.png" style="zoom:67%;" />

    <img src="https://s1.328888.xyz/2022/05/21/dqnh2.png" style="zoom:67%;" />

    ä¸¤è€…çš„ä¸åŒåŒ…æ‹¬ï¼špre-trainingç”¨unlabelledçš„èµ„æ–™ï¼Œè€ŒMAMLè®­ç»ƒç”¨åˆ°labelledèµ„æ–™ã€‚æ—©æœŸçš„self-supervised learningå®é™…ä¸Šä¼šæŠŠæ‰€æœ‰èµ„æ–™æ”¾ä¸€å—è®­ç»ƒä¸€ä¸ªmodelï¼Œè¿™ç§æ–¹æ³•ä¹Ÿç§°ä¹‹ä¸ºå¤šä»»åŠ¡å­¦ä¹ ï¼ˆmulti-task learningï¼‰

  - æˆ‘ä»¬ç”šè‡³å¯ä»¥è®¤ä¸ºä¸åŒçš„ä»»åŠ¡å°±æ˜¯ä¸åŒçš„domainï¼Œé‚£æˆ‘ä»¬åœ¨ä¸åŒçš„ä»»åŠ¡ä¸Šçš„meta learningæ˜¯ä¸€ç§è§£å†³domain adaptationçš„æ–¹æ³•ã€‚~~DAæ›´æ˜¯ä¸€ä¸ªé—®é¢˜è€Œéæ–¹æ³•ï¼Œå°è¯•è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æœ‰å¾ˆå¤šâ€¦~~

  æ›´å¤šå…³äºMAMLçš„â€¦

- ANIL (Almost No Inner Loop)

  > Aniruddh Raghu, Maithra Raghu, Samy Bengio, Oriol Vinyals, Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML, ICLR, 2020
  >
  > <img src="https://s1.328888.xyz/2022/05/21/dq1YM.png" style="zoom: 67%;" />
  >
  > Feature Reuseæ˜¯MAMLæ•ˆæœå¥½çš„å…³é”®ã€‚

- [Reptile](https://arxiv.org/abs/1803.02999)

- First order MAML (FOMAML)

  > å¤§å¹…ç®€åŒ–MAMLè¿ç®—

æ²¡ç»†è®²äº†ï¼Œæ³¨æ„åšä½œä¸šã€‚ovo

#### Learning a Optimizer

åœ¨updateå‚æ•°çš„æ—¶å€™ï¼ŒæŠŠä¼˜åŒ–å™¨å‚æ•°è‡ªåŠ¨å­¦ä¹ å‡ºæ¥ã€‚

<img src="https://s1.328888.xyz/2022/05/21/dqbM7.png" style="zoom: 67%;" />

åœ¨æ–‡ç« Marcin Andrychowicz, et al., Learning to learn by gradient descent by gradient descent, NIPS2016ä¸­ï¼Œå…¶ä¸­ADAMã€RMSpropã€SGDã€NAGéƒ½æ˜¯äººä¸ºåˆ¶ä½œçš„ï¼Œä½œè€…åŸºäºâ€œLSTMâ€è®¾è®¡äº†è‡ªåŠ¨åˆ¶ä½œOptimizerï¼Œæ•ˆæœå¦‚ä¸‹

<img src="https://s1.328888.xyz/2022/05/21/dqVfX.png" style="zoom:67%;" />

#### å­¦ç½‘ç»œæ¶æ„ï¼šNetwork Architecture Search (NAS)

<img src="https://s1.328888.xyz/2022/05/21/dqZpZ.png" style="zoom:67%;" />

æŠŠNetwork Structureå½“ä½œ$\phi$ï¼Œä¸è¿‡$âˆ‡_ğœ™ğ¿(ğœ™)$æ— æ³•è®¡ç®—ã€‚

- æ²¡æ³•ç®—å¾®åˆ†â€”â€”ç”¨Reinforcement Learningç¡¬åšæˆ–è®¸é˜”ä»¥ã€‚

> é˜…è¯»ææ–™æœ‰ï¼š
> Barret Zoph, et al., Neural Architecture Search with Reinforcement Learning, ICLR 2017
> Barret Zoph, et al., Learning Transferable Architectures for Scalable Image Recognition, CVPR, 2018
> Hieu Pham, et al., Efficient Neural Architecture Search via Parameter Sharing, ICML, 2018

An agent uses a set of actions to determine the network architecture.
å¾…å­¦ä¹ çš„ $\phi$ å°±æ˜¯agentçš„å‚æ•°ï¼Œåˆ™$-L(\phi)$å°±æ˜¯å»maximizeçš„rewardã€‚ç”¨RLç¡¬trainä¸€å‘â€¦ç¤ºä¾‹å¦‚ä¸‹

<img src="https://s1.328888.xyz/2022/05/21/dqjFC.png" style="zoom: 80%;" />

- ç”¨è¿›åŒ–ç®—æ³•ï¼ˆEvolution Algorithmï¼‰åšNAS

>Esteban Real, et al., Large-Scale Evolution of Image Classifiers, ICML 2017
>Esteban Real, et al., Regularized Evolution for Image Classifier Architecture Search, AAAI, 2019
>Hanxiao Liu, et al., Hierarchical Representations for Efficient Architecture Search, ICLR, 2018

- æŠ˜è…¾ç‚¹ï¼Œè®¾è®¡æ–¹æ³•è®©AESå¯å¾®åˆ†ï¼Œæ¥è‡ª[DARTS]()

  >Hanxiao Liu, et al., DARTS: Differentiable Architecture Search, ICLR, 2019
  >
  ><img src="https://s1.328888.xyz/2022/05/21/dq6Gg.png" style="zoom:67%;" />

#### Data Processing

<img src="https://s1.328888.xyz/2022/05/21/dqMl1.png" style="zoom:67%;" />

- å­¦Data Augmentationï¼Œè®©machineè‡ªåŠ¨å­¦å¦‚ä½•æ•°æ®å¢å¼º

  <img src="https://s1.328888.xyz/2022/05/21/dqNOt.png" style="zoom:67%;" />

  > é˜…è¯»ææ–™ï¼š
  >
  > Yonggang Li, Guosheng Hu,etc. DADA: Differentiable Automatic Data Augmentation, ECCV, 2020
  > Daniel Ho,etc. Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules, ICML, 2019
  > Ekin D. Cubuk, Barret Zoph,etc. AutoAugment: Learning Augmentation Policies from Data, CVPR, 2019

- ç»™ä¸åŒçš„sampleä¸åŒçš„æƒé‡ã€‚

  çœ‹æ³•å¤šç§å¤šæ ·ï¼Œç¦»boundè¿‘çš„exampleæœ‰äººè§‰å¾—å¾ˆéš¾åˆ†ç¦»ï¼Œæƒé‡å¤§ç‚¹ï¼›æœ‰äººè§‰å¾—è¿™æ˜¯å™ªéŸ³ï¼Œæƒé‡å°äº›ã€‚ç”¨meta learningæ¥è‡ªåŠ¨å†³å®šã€‚è¿™ä¸ªå°±æ˜¯***sample weighting strategies***é—®é¢˜

  >Jun Shu, Qi Xie, Lixuan Yi,  Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting, NeurIPS, 2019
  >Mengye Ren, Wenyuan Zeng, Learning to Reweight Examples for Robust Deep Learning, ICML, 201

#### Beyond GDï¼ˆå¾…å¡«çš„å‘ï¼‰

ä»¥ä¸Šçš„æ–¹æ³•éƒ½è¿˜æ˜¯åŸºäºgradient descentæ‰€åšçš„æ–¹æ³•ï¼Œå­¦å‡ºæ¥çš„componentã€‚åœ¨GDä¹‹å¤–ï¼Œä¸å¦¨è®©æ•´ä¸ªnetworkçš„å‚æ•°ä½œä¸º$\phi$ï¼Œå½»åº•è·‘å»æ¢¯åº¦ä¸‹é™ï¼Œå‘æ˜æ–°çš„ä¼˜åŒ–ç®—æ³•ï¼ˆlearning algorithmï¼‰ï¼›ç›®å‰æœ‰äº›ç ”ç©¶å¾€è¿™ä¸ªæ–¹å‘è¿›å±•â€¦â€¦ï¼ˆç»™çš„ä¸€ä¸ªå‘ï¼Œå¡«å¡«å¡«ï¼‰

> å¯å‚è€ƒé˜…è¯»Andrei A. Rusu, Dushyant Rao,Raia Hadsell, Meta-Learning with Latent Embedding Optimization, ICLR, 2019

<img src="https://s1.328888.xyz/2022/05/21/dqSxe.png" style="zoom:67%;" />

æŠŠä¸€æ¬¡training/testingï¼ˆå³ä¸€ä¸ªepisodeï¼‰åŒ…åœ¨ä¸€ä¸ªnetworké‡Œè¾¹ï¼šæŠŠtraining dataä¸¢è¿›å»ï¼Œå†ä¸¢è¿›testing dataï¼Œnetworkå°±ç›´æ¥ç»™å‡ºæµ‹è¯•èµ„æ–™çš„ç­”æ¡ˆã€‚æ¨¡ç³Šæ‰episodeé‡Œè¾¹trainå’Œtestçš„è¾¹ç•Œï¼Œæ”¾åœ¨ä¸€ä¸ªç½‘ç»œä¸­å®ç°ã€‚

<img src="https://s1.328888.xyz/2022/05/21/dqBhO.png" style="zoom:67%;" />

ğŸ‘†è¿™ç§æ–¹æ³•å·²æœ‰äº†ï¼Œè¯¦ç»†å¯å»äº†è§£**Learning to compare**(metric-based approach)

### åº”ç”¨

#### ç”¨meta learningå®ç°few shot learning

ä»¥Few-shot Image Classificationä¸ºä¾‹ï¼Œå¦‚æœæ¯ä¸€ä¸ªclasséƒ½åªæœ‰å°‘é‡çš„èµ„æ–™

> N-ways K-shot classification: æ¯ä¸ªä»»åŠ¡é‡Œè¾¹ï¼Œæœ‰Nä¸ªclassesï¼Œæ¯ä¸ªclassæœ‰Kä¸ªexample<img src="https://s1.328888.xyz/2022/05/21/dqETq.png" alt="image-20220521180625322" style="zoom:50%;" />

åœ¨meta learningä¸­æˆ‘ä»¬éœ€è¦å‡†å¤‡è®¸å¤šN-ways K-shotçš„ä»»åŠ¡æ¥ä½œä¸ºè®­ç»ƒå’Œæµ‹è¯•ä»»åŠ¡ã€‚æœ€å¸¸è§çš„é€”å¾„æ˜¯é€šè¿‡[**Omniglot**](https://github.com/brendenlake/omniglot)ã€‚æ€»å…±æœ‰1623ä¸ªcharacterï¼Œæ¯ä¸ªcharacteræœ‰20ä¸ªexample

ä»¥åˆ¶ä½œ20-ways -1shotä¸ºä¾‹ï¼Œåœ¨Omniglotä¸­é€‰æ‹©20ä¸ªcharacterï¼Œæ¯ä¸ªcharacteré€‰æ‹©ä¸€ä¸ªexample

<img src="https://s1.328888.xyz/2022/05/21/dqPMP.png" style="zoom:67%;" />

Split your characters into training and testing characters
- Sample N training characters, sample K examples from each sampled characters  â†’one training task
- Sample N testing characters, sample K examples from each sampled characters  â†’one testing task

#### meta learningåœ¨å…·ä½“é¢†åŸŸçš„å¯èƒ½åº”ç”¨

![image-20220521181309039](https://s1.328888.xyz/2022/05/21/dDQfm.png)

æ¥è‡ªhttp://speech.ee.ntu.edu.tw/~tlkagk/meta_learning_table.pdf